{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soild classifier train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pathlib\n",
    "import glob\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # self.batchSize = batch_size\n",
    "\n",
    "        # ((inputsize-kernel+2*Padding)/stride) +1\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=12, kernel_size=(3,3), stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(num_features=12),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=12, out_channels=20, kernel_size=(3,3), stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(num_features=20),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=20, out_channels=30, kernel_size=(3,3), stride=1, padding=1),\n",
    "            # output: 30,20,20\n",
    "            torch.nn.BatchNorm2d(num_features=30),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "\n",
    "        # Flatten the output before fully connected layers\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(in_features=30* 37* 37, out_features=128)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # print(\"conv1\",x.size())\n",
    "\n",
    "        x = self.maxpool1(x)\n",
    "        # print(\"maxpool1\",x.size())\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        # print(\"conv2\",x.size())\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        # print(\"conv3\",x.size())\n",
    "\n",
    "        x = self.maxpool2(x)\n",
    "        # print(\"maxpool2\",x.size())\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "        # print(\"flatten\",x.size())\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        # print(\"fc1\",x.size())\n",
    "\n",
    "        x = self.relu4(x)\n",
    "        # print(\"relu4\",x.size())\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        # print(\"fc2\",x.size())\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=3).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=41070, out_features=128, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(+90),\n",
    "    transforms.ToTensor(),  # 0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],  # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                         [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train'\n",
    "val_path = './data/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform=transformer),\n",
    "    batch_size=batchSize, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(val_path, transform=transformer),\n",
    "    batch_size=batchSize//2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gravel', 'Sand', 'Silt']\n"
     ]
    }
   ],
   "source": [
    "# categories\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "# CNN Network\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the size of training and testing images\n",
    "train_count = len(glob.glob(train_path + '/**/*.jpg'))\n",
    "test_count = len(glob.glob(val_path + '/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 147\n"
     ]
    }
   ],
   "source": [
    "print(train_count, test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5116536021232605\n",
      "0 0.04820380359888077\n",
      "0 0.30285438895225525\n",
      "0 0.778286874294281\n",
      "0 0.18381761014461517\n",
      "0 0.08878567069768906\n",
      "0 0.7207648754119873\n",
      "0 0.5898793935775757\n",
      "0 3.0445287227630615\n",
      "0 1.8412678241729736\n",
      "0 0.026245875284075737\n",
      "0 0.2089916467666626\n",
      "0 0.23708800971508026\n",
      "0 0.336184024810791\n",
      "0 0.0739688128232956\n",
      "0 0.009684115648269653\n",
      "0 0.14372915029525757\n",
      "0 3.2998499870300293\n",
      "0 0.10084935277700424\n",
      "0 0.26736047863960266\n",
      "0 0.24864265322685242\n",
      "0 0.05333418399095535\n",
      "0 0.26546990871429443\n",
      "0 0.0004816311993636191\n",
      "0 0.34140169620513916\n",
      "0 0.371107280254364\n",
      "0 0.15757186710834503\n",
      "0 0.03291301429271698\n",
      "0 0.013061122968792915\n",
      "0 1.512782335281372\n",
      "0 0.2963249683380127\n",
      "0 0.5363077521324158\n",
      "0 0.7796669006347656\n",
      "0 0.021962717175483704\n",
      "0 0.29563775658607483\n",
      "0 0.16298474371433258\n",
      "0 0.46173444390296936\n",
      "0 0.33888331055641174\n",
      "Epoch 0 - Validation Loss: 0.4430, Accuracy: 82.67%\n",
      "1 0.19365835189819336\n",
      "1 1.794156789779663\n",
      "1 0.7348855137825012\n",
      "1 0.42204946279525757\n",
      "1 0.11232858896255493\n",
      "1 0.042304981499910355\n",
      "1 0.21268481016159058\n",
      "1 0.00045923705329187214\n",
      "1 0.31777632236480713\n",
      "1 0.049427371472120285\n",
      "1 0.34187188744544983\n",
      "1 0.341115802526474\n",
      "1 2.540149450302124\n",
      "1 0.3706369996070862\n",
      "1 0.37150171399116516\n",
      "1 0.32544293999671936\n",
      "1 0.254755437374115\n",
      "1 0.22755631804466248\n",
      "1 0.4073212146759033\n",
      "1 0.006308328360319138\n",
      "1 1.518494725227356\n",
      "1 0.1852714717388153\n",
      "1 0.4027896821498871\n",
      "1 0.06763200461864471\n",
      "1 0.2041032463312149\n",
      "1 1.3953583240509033\n",
      "1 0.38588425517082214\n",
      "1 0.03562365472316742\n",
      "1 0.15507692098617554\n",
      "1 0.3300553858280182\n",
      "1 0.2179497629404068\n",
      "1 0.5629045963287354\n",
      "1 0.3649221658706665\n",
      "1 0.597500741481781\n",
      "1 0.2583777606487274\n",
      "1 0.5921632647514343\n",
      "1 0.1393376588821411\n",
      "1 0.004467397928237915\n",
      "Epoch 1 - Validation Loss: 0.2548, Accuracy: 87.33%\n",
      "2 0.6085646152496338\n",
      "2 0.06452051550149918\n",
      "2 0.1593095362186432\n",
      "2 1.6476799249649048\n",
      "2 0.32176750898361206\n",
      "2 0.28718018531799316\n",
      "2 0.23988047242164612\n",
      "2 0.10858647525310516\n",
      "2 0.16295698285102844\n",
      "2 0.18658483028411865\n",
      "2 0.48487749695777893\n",
      "2 0.40477731823921204\n",
      "2 0.7930903434753418\n",
      "2 0.7373714447021484\n",
      "2 0.10418049991130829\n",
      "2 0.2692677974700928\n",
      "2 0.3611888587474823\n",
      "2 0.14180608093738556\n",
      "2 0.36483991146087646\n",
      "2 0.03685656189918518\n",
      "2 0.2045050859451294\n",
      "2 0.3002205789089203\n",
      "2 0.12171205878257751\n",
      "2 0.2097657024860382\n",
      "2 0.44666868448257446\n",
      "2 0.0855027288198471\n",
      "2 0.6442242860794067\n",
      "2 0.2801728844642639\n",
      "2 0.17532974481582642\n",
      "2 0.023659497499465942\n",
      "2 0.05963514372706413\n",
      "2 0.15315398573875427\n",
      "2 0.4320414960384369\n",
      "2 0.031813062727451324\n",
      "2 0.015669384971261024\n",
      "2 0.6820807456970215\n",
      "2 0.6826960444450378\n",
      "2 0.39776700735092163\n",
      "Epoch 2 - Validation Loss: 0.2114, Accuracy: 86.67%\n",
      "3 0.12012269347906113\n",
      "3 0.07687236368656158\n",
      "3 0.482203871011734\n",
      "3 0.08378542214632034\n",
      "3 0.9850855469703674\n",
      "3 0.010297873988747597\n",
      "3 0.6068806052207947\n",
      "3 0.25208303332328796\n",
      "3 0.477577805519104\n",
      "3 0.03748825937509537\n",
      "3 0.7313721179962158\n",
      "3 0.1499186009168625\n",
      "3 0.05472172796726227\n",
      "3 0.40565088391304016\n",
      "3 1.3996847867965698\n",
      "3 0.29968857765197754\n",
      "3 0.517645001411438\n",
      "3 0.050560176372528076\n",
      "3 0.2189899981021881\n",
      "3 0.4915711283683777\n",
      "3 0.11682198941707611\n",
      "3 0.823047399520874\n",
      "3 0.0689815878868103\n",
      "3 0.07557447254657745\n",
      "3 1.054845929145813\n",
      "3 0.13612954318523407\n",
      "3 0.38963067531585693\n",
      "3 0.2709137201309204\n",
      "3 0.2017516940832138\n",
      "3 0.7449851632118225\n",
      "3 0.39241650700569153\n",
      "3 0.060299649834632874\n",
      "3 0.30378255248069763\n",
      "3 0.3112747073173523\n",
      "3 0.2739901840686798\n",
      "3 0.019678203389048576\n",
      "3 0.11089705675840378\n",
      "3 0.012392041273415089\n",
      "Epoch 3 - Validation Loss: 0.2458, Accuracy: 88.00%\n",
      "4 0.08195015788078308\n",
      "4 1.2001352310180664\n",
      "4 0.31480100750923157\n",
      "4 0.4015604257583618\n",
      "4 0.23082242906093597\n",
      "4 0.4902597963809967\n",
      "4 0.6390079855918884\n",
      "4 0.1819131076335907\n",
      "4 0.1978234201669693\n",
      "4 0.09716635197401047\n",
      "4 0.6243181228637695\n",
      "4 0.11414626985788345\n",
      "4 0.5193613171577454\n",
      "4 0.1638343632221222\n",
      "4 0.010123427957296371\n",
      "4 0.048910509794950485\n",
      "4 0.06179746612906456\n",
      "4 0.19875097274780273\n",
      "4 0.6442884802818298\n",
      "4 0.046582963317632675\n",
      "4 0.1343521773815155\n",
      "4 0.13640019297599792\n",
      "4 0.046428680419921875\n",
      "4 0.5252348184585571\n",
      "4 0.061640843749046326\n",
      "4 0.01816653273999691\n",
      "4 1.3821561336517334\n",
      "4 0.8573427200317383\n",
      "4 0.28874441981315613\n",
      "4 0.0687953531742096\n",
      "4 0.00812306348234415\n",
      "4 0.6316888332366943\n",
      "4 0.4730638861656189\n",
      "4 0.42726460099220276\n",
      "4 2.774946451187134\n",
      "4 1.3116673231124878\n",
      "4 0.05469459295272827\n",
      "4 0.03797457739710808\n",
      "Epoch 4 - Validation Loss: 0.3351, Accuracy: 84.67%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        loss = torch.nn.functional.cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "            val_output = model(X_val)\n",
    "            val_loss += torch.nn.functional.cross_entropy(val_output, y_val).item()\n",
    "\n",
    "            _, predicted = val_output.max(1)\n",
    "            total += y_val.size(0)\n",
    "            correct += predicted.eq(y_val).sum().item()\n",
    "\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch} - Validation Loss: {average_val_loss:.4f}, Accuracy: {100 * accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4772, Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "test_path = './data/test'\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, transform=transformer),\n",
    "    batch_size=batchSize, shuffle=True\n",
    ")\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "        test_output = model(X_test)\n",
    "        test_loss += torch.nn.functional.cross_entropy(test_output, y_test).item()\n",
    "\n",
    "        _, predicted = test_output.max(1)\n",
    "        total += y_test.size(0)\n",
    "        correct += predicted.eq(y_test).sum().item()\n",
    "\n",
    "    average_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f\"Test Loss: {average_test_loss:.4f}, Accuracy: {100 * accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
