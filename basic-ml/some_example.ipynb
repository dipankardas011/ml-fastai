{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "running example: train a two-layer ReLU network on random data with L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5936,  0.9675,  0.2479],\n",
       "        [-1.4069, -1.0570, -2.0190]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N is the number of data points\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2= torch.randn(H, D_out, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4898, -0.5194, -0.8267, -0.3929, -1.0933,  0.3315, -1.8265, -0.2080,\n",
       "          1.7972,  0.5134]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    h = x.mm(w1) # X matmul w1\n",
    "    h_relu = h.clamp(min=0) # use activation function to the hidden layer\n",
    "    y_pred = h_relu.mm(w2) # hidden matmult w2\n",
    "    loss = (y_pred - y).pow(2).sum() # sqaure sum loss\n",
    "\n",
    "    grad_y_pred = 2.0 * (y_pred - y) # it is just differentiation\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    w1 -= learning_rate* grad_w1\n",
    "    w2 -= learning_rate* grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5315, -2.2143, -0.8954, -0.6610, -0.6031,  0.8249,  0.6192,  1.2545,\n",
       "          0.3083, -0.2887,  0.8703,  1.5056, -1.0473,  0.3107,  0.7957,  0.3389,\n",
       "          1.0303, -0.7558, -1.6617,  0.1515,  2.2062,  0.5581,  0.5029,  1.4396,\n",
       "         -0.6068, -0.8064, -0.3301,  0.2012,  0.2661, -1.6686, -0.1905, -1.0241,\n",
       "          0.4694,  0.8759, -1.0050, -0.0091, -0.8329, -0.5159, -2.0456, -0.2293,\n",
       "         -1.1314, -0.9064,  0.6369,  0.8547,  0.3533, -1.0125, -0.3455, -1.1306,\n",
       "          1.4173, -1.8185,  0.6905, -1.1772, -1.4044,  0.9248,  0.0805,  0.6364,\n",
       "          0.0789,  0.6480,  0.9278,  1.0812,  0.7277,  0.2533, -1.5227,  0.5346,\n",
       "          1.4883,  2.3632,  1.1129, -3.0738,  0.4576,  0.6122, -1.4783, -0.6386,\n",
       "          0.0418,  0.1857,  0.2793,  0.7301,  0.7003, -0.9364,  1.6653,  2.4008,\n",
       "          1.0775, -0.7225, -0.0830,  2.0328, -0.8451,  0.6982, -0.7560,  0.9785,\n",
       "         -1.4675,  1.3310,  1.6482, -0.8352,  0.8862,  1.1501,  0.6076, -0.8103,\n",
       "         -1.6767, -0.2094, -0.0231,  0.0363]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3317,  0.1398, -0.7393, -0.8986, -0.6101,  0.2838, -1.8702, -0.2033,\n",
       "          1.4925,  0.6795]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets use autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N is the number of data points\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2= torch.randn(H, D_out, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2342,  1.2281, -1.5796,  0.3111, -1.3187, -0.3890,  1.4018, -2.7042,\n",
       "         -1.0479, -0.2453]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate*w1.grad\n",
    "        w2 -= learning_rate*w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1197,  0.8572, -1.6828,  0.1270, -1.3622, -0.8218,  1.3890, -2.3421,\n",
       "         -1.1848, -0.7086]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+(-x).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5747.49072265625\n",
      "50 nan\n",
      "100 nan\n",
      "150 nan\n",
      "200 nan\n",
      "250 nan\n",
      "300 nan\n",
      "350 nan\n",
      "400 nan\n",
      "450 nan\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = sigmoid(x.mm(w1)).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate*w1.grad\n",
    "        w2 -= learning_rate*w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Caution: the sigmod python function is not the best way for that you can checks the notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N is the number of data points\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate*param.grad\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now optim: adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, D_in, H, D_out = 64, 1000, 100, 10 # N is the number of data points\n",
    "\n",
    "# x = torch.randn(N, D_in, device=device)\n",
    "# y = torch.randn(N, D_out, device=device)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "loader = DataLoader(TensorDataset(x, y), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in, H,D_out)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(20):\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
